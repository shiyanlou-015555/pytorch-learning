{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./data/Comments/train.csv')\n",
    "test = pd.read_csv('./data/Comments/test.csv')\n",
    "sample_submission = pd.read_csv('./data/Comments/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>酸菜鱼不错</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>轻食素食都是友善的饮食方式</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>完爆中午吃的农家乐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>烤鱼很入味</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>有种入口即化的感觉</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           text\n",
       "0      0          酸菜鱼不错\n",
       "1      0  轻食素食都是友善的饮食方式\n",
       "2      0      完爆中午吃的农家乐\n",
       "3      1          烤鱼很入味\n",
       "4      0      有种入口即化的感觉"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>理由很简单</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>蘸着花生酱吃非常美味</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>味道奶香味恰到好处</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>面包片烤的恰到好处</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>属于简单经济型</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text\n",
       "0       理由很简单\n",
       "1  蘸着花生酱吃非常美味\n",
       "2   味道奶香味恰到好处\n",
       "3   面包片烤的恰到好处\n",
       "4     属于简单经济型"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.480232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.362658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.148902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.105988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.285303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Prediction\n",
       "0   0    0.480232\n",
       "1   1    0.362658\n",
       "2   2    0.148902\n",
       "3   3    0.105988\n",
       "4   4    0.285303"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/ml/gyc9l97n0cq3pfrr93x8xmy80000gn/T/jieba.cache\n",
      "Loading model cost 0.718 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "# NN模型不用去除stop_words\n",
    "\n",
    "corpus_train = []\n",
    "for one in train['text']:\n",
    "    mid = []\n",
    "    for ele in list(jieba.cut(one, cut_all=False, HMM=True)):\n",
    "        mid.append(ele)\n",
    "    corpus_train.append(' '.join(mid))\n",
    "    \n",
    "corpus_test = []\n",
    "for one in test['text']:\n",
    "    mid = []\n",
    "    for ele in list(jieba.cut(one, cut_all=False, HMM=True)):\n",
    "        mid.append(ele)\n",
    "    corpus_test.append(' '.join(mid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['酸菜鱼 不错',\n",
       " '轻食 素食 都 是 友善 的 饮食 方式',\n",
       " '完爆 中午 吃 的 农家乐',\n",
       " '烤鱼 很 入味',\n",
       " '有种 入口 即化 的 感觉',\n",
       " '菜品 一如既往 的 好',\n",
       " '味道 非常 好',\n",
       " '团购 很 优惠',\n",
       " '咖喱 牛腩 不错',\n",
       " '部分 菜 偏 酸辣 口']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['理由 很 简单',\n",
       " '蘸 着 花生酱 吃 非常 美味',\n",
       " '味道 奶 香味 恰到好处',\n",
       " '面包片 烤 的 恰到好处',\n",
       " '属于 简单 经济型',\n",
       " '芝士 鸡蛋 卷 一如既往 的 好吃 香香 嫩嫩的',\n",
       " '去 宁波 科技馆 非常 方便',\n",
       " '味道 一如既往 的 好',\n",
       " '兔头 真 好吃',\n",
       " '装修 风格 独特']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 9511\n"
     ]
    }
   ],
   "source": [
    "def get_vocab_comments(data):\n",
    "    '''\n",
    "    @params:\n",
    "        data: corpus_train or corpus_test\n",
    "    @return: 数据集上的词典，Vocab 的实例（freqs, stoi, itos）\n",
    "    '''\n",
    "    counter = collections.Counter([word for sentence in data for word in sentence.split(' ')])\n",
    "    return Vocab.Vocab(counter, min_freq=1)\n",
    "\n",
    "vocab = get_vocab_comments(corpus_train)  # 训练数据\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comments(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    '''\n",
    "    max_l = 32  # 统一文本长度为500\n",
    "\n",
    "    def pad(x):  # 将每条评论通过截断或者补1 (<pad>)，使得长度变成500\n",
    "        return x[:max_l] if len(x) > max_l else x + [1] * (max_l - len(x))\n",
    "\n",
    "    # vocab.stoi(word) 将word转成index\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words.split(' ')]) for words in data])\n",
    "    labels = torch.tensor(train['label'])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 32]) y torch.Size([64])\n",
      "tensor([  47,   35, 2386, 1686, 2117,    7,    4,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1])\n",
      "#batches: 250\n"
     ]
    }
   ],
   "source": [
    "train_set = Data.TensorDataset(*preprocess_comments(corpus_train, vocab))\n",
    "\n",
    "batch_size = 64\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    print(X[0])\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @params:\n",
    "            x: 输入，形状为 (batch_size, n_channels, seq_len) 的张量\n",
    "        @return: 时序最大池化后的结果，形状为 (batch_size, n_channels, 1) 的张量\n",
    "        '''\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) # kenerl_size=seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        '''\n",
    "        @params:\n",
    "            vocab: 在数据集上创建的词典，用于获取词典大小\n",
    "            embed_size: 嵌入维度大小\n",
    "            kernel_sizes: 卷积核大小列表\n",
    "            num_channels: 卷积通道数列表\n",
    "        '''\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size) # 参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size) # 不参与训练的嵌入层\n",
    "        \n",
    "        self.pool = GlobalMaxPool1d() # 时序最大池化层没有权重，所以可以共用一个实例\n",
    "        self.convs = nn.ModuleList()  # 创建多个一维卷积层\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                                        out_channels = c, \n",
    "                                        kernel_size = k))\n",
    "            \n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        self.dropout = nn.Dropout(0.5) # 丢弃层用于防止过拟合\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 词语下标序列，形状为 (batch_size, seq_len) 的整数张量\n",
    "        @return:\n",
    "            outputs: 对文本情感的预测，形状为 (batch_size, 2) 的张量\n",
    "        '''\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2) # (batch_size, seq_len, 2*embed_size)\n",
    "        # 根据一维卷积层要求的输入格式，需要将张量进行转置\n",
    "        embeddings = embeddings.permute(0, 2, 1) # (batch_size, 2*embed_size, seq_len)\n",
    "        \n",
    "        encoding = torch.cat([\n",
    "            self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        \n",
    "        # 应用丢弃法后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(9511, 100)\n",
       "  (constant_embedding): Embedding(9511, 100)\n",
       "  (pool): GlobalMaxPool1d()\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(200, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): Conv1d(200, 100, kernel_size=(4,), stride=(1,))\n",
       "    (2): Conv1d(200, 100, kernel_size=(5,), stride=(1,))\n",
       "  )\n",
       "  (decoder): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            # forward\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y) \n",
    "            \n",
    "            # backward 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.4135, train acc 0.812, test acc 0.888, time 26.6 sec\n",
      "epoch 2, loss 0.1529, train acc 0.873, test acc 0.926, time 26.3 sec\n",
      "epoch 3, loss 0.0771, train acc 0.907, test acc 0.948, time 25.8 sec\n",
      "epoch 4, loss 0.0468, train acc 0.926, test acc 0.964, time 26.3 sec\n",
      "epoch 5, loss 0.0296, train acc 0.945, test acc 0.967, time 24.5 sec\n",
      "epoch 6, loss 0.0200, train acc 0.954, test acc 0.977, time 26.5 sec\n"
     ]
    }
   ],
   "source": [
    "### 训练并评价模型\n",
    "\n",
    "lr, num_epochs = 0.001, 6\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, train_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整测试数据\n",
    "def preprocess_test_comments(data, vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        data: 同上，原始的读入数据\n",
    "        vocab: 训练集上生成的词典\n",
    "    @return:\n",
    "        features: 单词下标序列，形状为 (n, max_l) 的整数张量\n",
    "        labels: 情感标签，形状为 (n,) 的0/1整数张量\n",
    "    '''\n",
    "    max_l = 32  # 统一文本长度为500\n",
    "\n",
    "    def pad(x):  # 将每条评论通过截断或者补1 (<pad>)，使得长度变成500\n",
    "        return x[:max_l] if len(x) > max_l else x + [1] * (max_l - len(x))\n",
    "\n",
    "    # vocab.stoi(word) 将word转成index\n",
    "    features = torch.tensor([pad([vocab.stoi[word] for word in words.split(' ')]) for words in data])\n",
    "    # labels = torch.tensor(train['label'])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4189, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = preprocess_test_comments(corpus_test, vocab)\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(9511, 100)\n",
       "  (constant_embedding): Embedding(9511, 100)\n",
       "  (pool): GlobalMaxPool1d()\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv1d(200, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): Conv1d(200, 100, kernel_size=(4,), stride=(1,))\n",
       "    (2): Conv1d(200, 100, kernel_size=(5,), stride=(1,))\n",
       "  )\n",
       "  (decoder): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4189, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = net(test_set.to(device))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['Prediction'] = F.softmax(res.cpu(), dim=1).detach().numpy()[:, 1]\n",
    "\n",
    "res_file = './data/Comments/TextCNN_submission_softmax_lre_3_epoch6.csv'\n",
    "sample_submission.to_csv(res_file, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
