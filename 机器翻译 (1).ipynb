{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECE26A1028E24B3099AD6F7D0CECEEE1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 机器翻译和数据集\n",
    "\n",
    "\n",
    "机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。\n",
    "主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "431E9AECF60849679FA0CDB3B39C25D5",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fraeng6506', 'd2l9528']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/home/kesci/input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F99F306F631B4E2F8F3464EEE0234CE1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/kesci/input/d2l9528/')\n",
    "import collections\n",
    "import d2l\n",
    "import zipfile\n",
    "from d2l.data.base import Vocab\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F# 包装好的类\n",
    "from torch.utils import data\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DF1C65FE8E3493F83CCAD6D9CAD5E4A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 数据预处理\n",
    "将数据集清洗、转化为神经网络的输入minbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C4C3229D46004ECF93F03C6BBC48AEDC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
      "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
      "Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
      "Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
      "Who?\tQui ?\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)\n",
      "Wow!\tÇa alors !\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)\n",
      "Fire!\tAu feu !\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)\n",
      "Help!\tÀ l'aide !\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\n",
      "Jump.\tSaute.\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix)\n",
      "Stop!\tÇa suffit !\tCC-BY 2.0 (France) Attribution: tato\n",
      "25666314\n"
     ]
    }
   ],
   "source": [
    "with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n",
    "      raw_text = f.read()\n",
    "print(raw_text[0:1000])#使用英语法语的tx\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BB101036A28849798E95206AAE93FDFC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\tcc-by 2 .0 (france) attribution: tatoeba .org #2877272 (cm) & #1158250 (wittydev)\n",
      "hi .\tsalut !\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #509819 (aiji)\n",
      "hi .\tsalut .\tcc-by 2 .0 (france) attribution: tatoeba .org #538123 (cm) & #4320462 (gillux)\n",
      "run !\tcours !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906331 (sacredceltic)\n",
      "run !\tcourez !\tcc-by 2 .0 (france) attribution: tatoeba .org #906328 (papabear) & #906332 (sacredceltic)\n",
      "who?\tqui ?\tcc-by 2 .0 (france) attribution: tatoeba .org #2083030 (ck) & #4366796 (gillux)\n",
      "wow !\tça alors !\tcc-by 2 .0 (france) attribution: tatoeba .org #52027 (zifre) & #374631 (zmoo)\n",
      "fire !\tau feu !\tcc-by 2 .0 (france) attribution: tatoeba .org #1829639 (spamster) & #4627939 (sacredceltic)\n",
      "help !\tà l'aide !\tcc-by 2 .0 (france) attribution: tatoeba .org #435084 (lukaszpp) & #128430 (sysko)\n",
      "jump .\tsaute .\tcc-by 2 .0 (france) attribution: tatoeba .org #631038 (shishir) & #2416938 (phoenix)\n",
      "stop !\tça suffit !\tcc-b\n"
     ]
    }
   ],
   "source": [
    "def preprocess_raw(text):\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    # \\xa0 是不间断空白符 &nbsp;\n",
    "    # \\u202f 是空白字符\n",
    "    out = ''\n",
    "    for i, char in enumerate(text.lower()):\n",
    "        if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
    "            out += ' '\n",
    "        out += char\n",
    "    return out\n",
    "\n",
    "text = preprocess_raw(raw_text)\n",
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F249A8DDE8E41EA8BF5DF0C62196D30",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "字符在计算机里是以编码的形式存在，我们通常所用的空格是 \\x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。\n",
    "而 \\xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F005D61B217439D988A086B843DF940",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 分词\n",
    "字符串---单词组成的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AC2D82A7081943B0BE94018061BC62AF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['go', '.'], ['hi', '.'], ['hi', '.']],\n",
       " [['va', '!'], ['salut', '!'], ['salut', '.']])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = 50000\n",
    "source, target = [], []\n",
    "for i, line in enumerate(text.split('\\n')):\n",
    "    if i > num_examples:\n",
    "        break\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) >= 2:\n",
    "        source.append(parts[0].split(' '))\n",
    "        target.append(parts[1].split(' '))\n",
    "        # 将每个单词分开\n",
    "        \n",
    "source[0:3], target[0:3]\n",
    "# source是英语,target是法语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7589E7D345B3463A8F0F4574ED6EDA9A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/7589E7D345B3463A8F0F4574ED6EDA9A/q5sns19ttq.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 统计句长\n",
    "d2l.set_figsize()\n",
    "d2l.plt.hist([[len(l) for l in source], [len(l) for l in target]],label=['source', 'target'])\n",
    "# 纵坐标是频数\n",
    "d2l.plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9ABB7E025F14892BD21C7ADA88F848C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 建立词典\n",
    "单词组成的列表---单词id组成的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D432CD6C4A644196831393AEF3EF6A06",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokens):\n",
    "    tokens = [token for line in tokens for token in line]\n",
    "    # 词汇表,还没有进行counter\n",
    "    # print(tokens)\n",
    "    return d2l.data.base.Vocab(tokens, min_freq=3, use_special_tokens=True)# 自己的一个函数\n",
    "# tokens:所有单词组成的一个列表,min_fre=3:只有出现频次大于3的才能够出现在词典中\n",
    "# use_special_tokens:是否使用特殊字符\n",
    "src_vocab = build_vocab(source)\n",
    "len(src_vocab)\n",
    "print(src_vocab['.']) \n",
    "print(src_vocab.pad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B1461B583FF94E978D53F05EB8EFCFB5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jc5ga5gy.png?imageView2/0/w/960/h/960)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF4EFEF2AB424A5587358006DE570FAA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "E556507EFE624C93B9A181F68B2CF64A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[38, 4, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad(line, max_len, padding_token):\n",
    "    if len(line) > max_len:\n",
    "        return line[:max_len]\n",
    "    return line + [padding_token] * (max_len - len(line))\n",
    "print(source[0])\n",
    "pad(src_vocab[source[0]], 10, src_vocab.pad)# pad填充,列表不够长的时候直接截取,长的符号直接截断\n",
    "#src_vocab[source[0]]将数据char -> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E704BB99768F474E81871942B81E0665",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_array(lines, vocab, max_len, is_source):\n",
    "    lines = [vocab[line] for line in lines]# 将每一句话转化为数组\n",
    "    if not is_source:\n",
    "        #判断这个句子是英语还是法语，如果是英语也就是target需要进行包装\n",
    "        lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n",
    "    array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n",
    "    # 将每一句话扩充\n",
    "    valid_len = (array != vocab.pad).sum(1) #第一个维度,有效长度,原来的长度\n",
    "    # 统计有意义的个数,每行,也就是每个样本有意义的个数\n",
    "    # print()\n",
    "    return array, valid_len\n",
    "    # 好处在于后面计算损失的时候直接拿有效长度进行误差分析就好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF66F496328148489E11D0D826291CB6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jc6e5tt1.png?imageView2/0/w/960/h/960)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7BB02BF246AC4302824AA6CFA95A40D8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_nmt(batch_size, max_len): # This function is saved in d2l.\n",
    "# 数据生成器\n",
    "    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)# 对数据和目标构建字典\n",
    "    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)#得到向量和每个样本有多少有用的字\n",
    "    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)# 得到目标的向量和每个样本有多少有用的\n",
    "    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    # 判断这4个东西是不是一一对应的,第0维度是不是相同的,也就是有多少句子是不是对应的\n",
    "    # 新版的tensorDataset是可变参数\n",
    "    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "    return src_vocab, tgt_vocab, train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DF377DA83D8C409CBC7A48ABDC3CCFDB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tensor([[ 62,  57, 100,  10, 111,   4,   0,   0],\n",
      "        [ 31,  83,   7, 382,   4,   0,   0,   0]], dtype=torch.int32) \n",
      "Valid lengths for X = tensor([6, 5]) \n",
      "Y = tensor([[   1, 1586,  139,   82,  155,   13,    2,    0],\n",
      "        [   1,   50, 1119,   12,  322,    4,    2,    0]], dtype=torch.int32) \n",
      "Valid lengths for Y = tensor([7, 7])\n"
     ]
    }
   ],
   "source": [
    "src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size=2, max_len=8)\n",
    "for X, X_valid_len, Y, Y_valid_len, in train_iter:\n",
    "    print('X =', X.type(torch.int32), '\\nValid lengths for X =', X_valid_len,\n",
    "        '\\nY =', Y.type(torch.int32), '\\nValid lengths for Y =', Y_valid_len)\n",
    "    break\n",
    "# 每次生成一组"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A2C40BD7BF1942A48EFDCE97B0C55D30",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Encoder-Decoder(编码解码,编码成隐藏状态)\n",
    " encoder：输入到隐藏状态  \n",
    " decoder：隐藏状态到输出\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640)\n",
    "## The Encoder\n",
    "encoder部分利用RNN压缩表示的性质。首先将源语言句子的每个词表示成一个向量，这个向量的维度与词汇表大小V相同，并且向量只有一个维度有值1，其余全都是0，1的位置就对应该词在词汇表的位置。这样的向量通常被称为one-hot向量，或者说1-of-K coding，它与词典的词一一对应，可以唯一的表示一个词，但是这样的向量实用，因为：\n",
    "\n",
    "向量的维度往往很大，容易造成维度灾难，\n",
    "无法刻画词与词之间的关系（例如语义相似性，也就是无法很好的表达语义）\n",
    "所以接下来要做的就是将每个词映射到一个低纬度的语义空间，每个词将由一个固定维度的稠密向量表示（也成分布表示Distributed Representation），也就是词向量。词向量的维度K，通常取100到500之间。词向量在整个翻译系统的训练过程中也会逐步更新，会变得更“meaningful”。\n",
    "我们知道词向量在语言学上有一定含义，这个含义无法直观的解释，但是向量之间的距离在一定程度上可以衡量词的相似性。模型的向量维度至少都在100维以上，无法直接画在纸上或者显示在屏幕上，可以使用PCA，T-SNE等方法映射到低维空间，压缩的向量确实可以保存源语言的句子的语义信息，因为语义越相近，句子在空间的距离就越近，这是传统的词袋模型（bag-of-words）所挖掘不出的信息。例如调换主语和宾语被认为语义不相近的句子。\n",
    "## The Decoder\n",
    "decoder部分同样适用RNN实现，\n",
    "\n",
    "encoder-decoder模型虽然非常经典，但是局限性也非常大。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量C。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中去。但是这样做有两个弊端，一是语义向量无法完全表示整个序列的信息，还有就是先输入的内容携带的信息会被后输入的信息稀释掉，或者说，被覆盖了。输入序列越长，这个现象就越严重。这就使得在解码的时候一开始就没有获得输入序列足够的信息，\n",
    "那么解码的准确度自然也就要打个折扣了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "86647F7276B743F1B4D466AF8055C27A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "A894858F5FA94C3599B3CE9D4EF88964",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "26FFA2FB4ED04DC6BA3BAAA4A522217C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2296785759574D6C86BD5DFBBC0A2E90",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以应用在对话系统、生成式任务中。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "857C6A5FDBA64866B66A42F8FD1A6A88",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Sequence to Sequence模型\n",
    "\n",
    "### 模型：\n",
    "训练  \n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640)\n",
    "预测\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "\n",
    "\n",
    "### 具体结构：\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500)\n",
    "# 总结\n",
    "- 实际上我们可以这么理解,encoder得到的输出就是decoder的输入,而这个输入就是当做循环神经网络的隐藏层,而decoder的输入x就是处理过后的target\n",
    "- 而预测的时候,decoder上一层的输入比如这里的bonjour就是下一层的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7886803BFCBC4D85A79A20B43FC5711A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    # 加码\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens=num_hiddens# 隐藏层\n",
    "        self.num_layers=num_layers# 层数\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)#10*.8\n",
    "        # 一个保存了固定字典和大小的简单查找表。这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。\n",
    "        # num_embeddings (int) - 嵌入字典的大小\n",
    "        #embedding_dim (int) - 每个嵌入向量的大小\n",
    "        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)# 深度lstm\n",
    "   # 使用lstm进行处\n",
    "    def begin_state(self, batch_size, device):\n",
    "        # 中间隐藏层变量,层数*批数*隐藏层\n",
    "        #2*8*16\n",
    "        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),\n",
    "                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]\n",
    "    def forward(self, X, *args):\n",
    "        #4*7*8\n",
    "        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)\n",
    "        # 考虑到输入是时序输入,所以句子的维度需要调换一下: 7*4\n",
    "        # print(X.shape)\n",
    "        X = X.transpose(0, 1)  # RNN needs first axes to be time\n",
    "        # state = self.begin_state(X.shape[1], device=X.device)\n",
    "        out, state = self.rnn(X)# 因为有两层\n",
    "        # The shape of out is (seq_len, batch_size, num_hiddens).7*4*16\n",
    "        # state contains the hidden state and the memory cell隐藏层和记忆细胞\n",
    "        # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "F48A7CA0591641A5A0AC12882EA3DA88",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\n",
    "# 10个字典,嵌入向量8个,2层,隐藏层参数16\n",
    "X = torch.zeros((4, 7),dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape, len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9CE0E8FBC8A4F7389B173DD62206CD5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "75A71227061547E0AF5EA9A5EDAE77F3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    #解码\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)# 字典大小,这个向量有所少维度\n",
    "# 一个保存了固定字典和大小的简单查找表。这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。\n",
    "        # num_embeddings (int) - 嵌入字典的大小\n",
    "        #embedding_dim (int) - 每个嵌入向量的大小\n",
    "        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens,vocab_size)# 所以输出10个\n",
    "#每个循环神经单元都需要输出,全连接层\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).transpose(0, 1)# 将刚才颠倒的维度颠倒回来\n",
    "        # print(X.shape)\n",
    "        out, state = self.rnn(X, state)\n",
    "        # Make the batch to be the first dimension to simplify loss computation.\n",
    "        out = self.dense(out).transpose(0, 1)\n",
    "        return out, state\n",
    "        # 4*7*10 单词表大小,怎么理解呢?一维度和二维度是4*7,最后一维度代表这个词语的输出有10个结果需要筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8B922C6FCF5643CFA46ECB509A109C8B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), 2, torch.Size([2, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)\n",
    "state = decoder.init_state(encoder(X))# 这里能直接把隐藏层参数拿过来用\n",
    "out, state = decoder(X, state)\n",
    "out.shape, len(state), state[0].shape, state[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E7FF047303445E881C9C7617FC845A9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "28493F1E5E1B47CA9FD60AF2AF1E2491",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  padding部分损失无效\n",
    "def SequenceMask(X, X_len,value=0):\n",
    "    '''\n",
    "    x:输出变量\n",
    "    x:len 输出变量的有效数字,通过target进行\n",
    "    '''\n",
    "    maxlen = X.size(1)\n",
    "    # 也需要放进去,不是通过device生成\n",
    "    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   \n",
    "   # print(mask)\n",
    "    X[~mask]=value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EAAF6E804ADA42A8892B4445F503E004",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2]])\n",
      "tensor([[0, 1, 2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(torch.tensor([1,2])[:, None] )\n",
    "print(torch.arange(3)[None, :].to(torch.tensor([1,2]).device))\n",
    "SequenceMask(X,torch.tensor([1,2]))\n",
    "# 去掉填充词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8AA5E43A0A8F4FD38C6E5AF16A747C6C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2,3, 4))\n",
    "SequenceMask(X, torch.tensor([1,2]),value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "2FA64864BA224A49B54C188323F7044B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    # pred shape: (batch_size, seq_len, vocab_size)decoder训练得分\n",
    "    # label shape: (batch_size, seq_len)\n",
    "    # valid_length shape: (batch_size, )\n",
    "    def forward(self, pred, label, valid_length):\n",
    "        # the sample weights shape should be (batch_size, seq_len)\n",
    "        weights = torch.ones_like(label)# weight全为1\n",
    "        weights = SequenceMask(weights, valid_length).float()# 把非有效位置变成0\n",
    "        self.reduction='none'\n",
    "        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)# 轴变换\n",
    "        return (output*weights).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "F59552886EC2485E8567BCD3C33F6D06",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.7269, 0.0000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones((3, 4, 10)), torch.ones((3,4),dtype=torch.long), torch.tensor([4,3,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8BFAA3559C240B6AFF8D4C053CCEE44",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "158F3AF8015D44F6891C579EAE3AC2F6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ch7(model, data_iter, lr, num_epochs, device):  # Saved in d2l\n",
    "# device 放入model,放入输入\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    tic = time.time()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        l_sum, num_tokens_sum = 0.0, 0.0\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]\n",
    "            # y_input = bos word eos\n",
    "            # y_label = word eos\n",
    "            # y_vlen = word\n",
    "            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1\n",
    "            \n",
    "            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n",
    "            l = loss(Y_hat, Y_label, Y_vlen).sum()\n",
    "            l.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                d2l.grad_clipping_nn(model, 5, device)\n",
    "                #随机裁剪梯度\n",
    "            num_tokens = Y_vlen.sum().item()\n",
    "            optimizer.step()\n",
    "            l_sum += l.sum().item()\n",
    "            num_tokens_sum += num_tokens\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch {0:4d},loss {1:.3f}, time {2:.1f} sec\".format( \n",
    "                  epoch, (l_sum/num_tokens_sum), time.time()-tic))\n",
    "            tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "94BC6BAA041A46DD9BEFD7BADFB32442",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   50,loss 0.093, time 27.4 sec\n",
      "epoch  100,loss 0.042, time 27.7 sec\n",
      "epoch  150,loss 0.030, time 27.6 sec\n",
      "epoch  200,loss 0.026, time 27.5 sec\n",
      "epoch  250,loss 0.025, time 27.6 sec\n",
      "epoch  300,loss 0.024, time 27.5 sec\n"
     ]
    }
   ],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
    "#没有随机丢单元\n",
    "batch_size, num_examples, max_len = 64, 1e3, 10\n",
    "lr, num_epochs, ctx = 0.005, 300, d2l.try_gpu()\n",
    "src_vocab, tgt_vocab, train_iter = d2l.load_data_nmt(\n",
    "    batch_size, max_len,num_examples)\n",
    "    # 导入数据\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(\n",
    "    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "    #初始化编码器和译码器\n",
    "model = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_ch7(model, train_iter, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F4C5D893110439FB07B542D4C161ECD",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "13CC26D191FF43BF8BB79FB14CDF7BAF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')]# id列表\n",
    "    src_len = len(src_tokens)\n",
    "    if src_len < max_len:\n",
    "        src_tokens += [src_vocab.pad] * (max_len - src_len)\n",
    "    enc_X = torch.tensor(src_tokens, device=device)\n",
    "    enc_valid_length = torch.tensor([src_len], device=device)\n",
    "    #enc_X已经获得了source\n",
    "    # 而enc_valid_length显示的则是有用的数值\n",
    "    # use expand_dim to add the batch_size dimension.\n",
    "    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length)# 增加到batchsize\n",
    "    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)\n",
    "    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0)\n",
    "    predict_tokens = []\n",
    "    for _ in range(max_len):\n",
    "        Y, dec_state = model.decoder(dec_X, dec_state)\n",
    "        # The token with highest score is used as the next time step input.\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        py = dec_X.squeeze(dim=0).int().item()\n",
    "        # print(py)\n",
    "        if py == tgt_vocab.eos:\n",
    "            break\n",
    "        predict_tokens.append(py)\n",
    "    return ' '.join(tgt_vocab.to_tokens(predict_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "D978A9B7B69B4E99942AE11491A7273B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go . => va !\n",
      "Wow ! => <unk> !\n",
      "I'm OK . => ça va .\n",
      "I won ! => j'ai gagné !\n"
     ]
    }
   ],
   "source": [
    "for sentence in ['Go .', 'Wow !', \"I'm OK .\", 'I won !']:\n",
    "    print(sentence + ' => ' + translate_ch7(\n",
    "        model, sentence, src_vocab, tgt_vocab, max_len, ctx))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B37CC2560DBC41A786C01BB5AB52CBFD",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Beam Search\n",
    "简单greedy search：只考虑局部最优解\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jchqoppn.png?imageView2/0/w/440/h/440)\n",
    "\n",
    "维特比算法：选择整体分数最高的句子（搜索空间太大）\n",
    "集束搜索：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6CFEE200E5D42E2BD3334E612C8A41A",
    "jupyter": {},
    "mdEditEnable": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "数据预处理\n",
    "1. 读取数据，处理数据中的编码问题，并将无效的字符串删除\n",
    "2. 分词，分词的目的就是将字符串转换成单词组成的列表。目前有很多现成的分词工具可以直接使用，也可以直接按照空格进行分词(不推荐，因为分词不是很准确)\n",
    "3. 建立词典，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西\n",
    "   1. 去重后词典，及其中单词对应的索引列表\n",
    "   2. 还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典。\n",
    "   3. 原始语料所有词对应的词典索引的列表\n",
    "4. 对数据进行padding操作,选取适合的长度\n",
    "5. 制作数据生成器，但是需要注意的是对于翻译任务的数据格式，机器翻译的输入是一段文本序列，输出也是一段文本序列。\n",
    "\n",
    "Seq2Seq模型的构建\n",
    "1. Seq2Seq模型由很多钟，但是整体框架都是基于先编码后解码的框架。也就是先对输入序列使用循环神经网络对他进行编码，编码成一个向量之后，再将编码得到的向量作为一个新的解码循环神经网络的隐藏状态的输入，进行解码，一次输出一个序列的元素，再将模型训练输出的序列元素与真实标签计算损失进行学习。\n",
    "2. 词嵌入，一般情况下输入到编码网络中的数据不是一个onehot向量而是经过了编码之后的向量，比如由word2vec技术，让编码后的向量由更加丰富的含义。\n",
    "3. 在进行编码和解码的过程中数据都是以时间步展开，也就是(Seq_len,)这种形式的数据进行处理的\n",
    "4. 对于编码与解码的循环神经网络，可以通过控制隐藏层的层数及每一层隐藏层神经元的数量来控制模型的复杂度\n",
    "5. 编码部分，RNN的用0初始化隐含状态，最后的输出主要是隐藏状态,编码RNN输出的隐含状态认为是其对应的编码向量\n",
    "6. 解码器的整体形状与编码器是一样的，只不过解码器的模型的隐藏状态是由编码器的输出的隐藏状态初始化的。\n",
    "\n",
    "损失函数\n",
    "1. 解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择分数最大的那个词作为最终的输出。\n",
    "2. 在计算损失函数之前，要把padding去掉，因为padding的部分不参与计算\n",
    "\n",
    "测试\n",
    "1. 解码器在测试的时候需要将模型的输出作为下一个时间步的输入\n",
    "2. Beam Search搜索算法。\n",
    "   1. 假设预测的时候词典的大小为3，内容为a,b,c. beam size为2，解码的时候过程如下\n",
    "   2. 生成第一个词的时候，选择概率最大的两个词，假设为a,c.那么当前的两个序列就是a和c。\n",
    "   3. 生成第二个词的时候，将当前序列a和c，分别与此表中的所有词进行组合，得到新的6个序列aa ab ac ca cb cc,计算每个序列的得分，并选择得分最高的2个序列，作为新的当前序列，假如为aa cb \n",
    "   4. 后面不断重复这个过程，直到遇到结束符或者达到最大长度为止，最终输出得分最高的2个序列。\n",
    "- Encoder-Decoder常应用于输入序列和输出序列的长度是可变的，如选项一二四，而分类问题的输出是固定的类别，不需要使用Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "933A87E6360C491D8D0BFD27C616A564",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
