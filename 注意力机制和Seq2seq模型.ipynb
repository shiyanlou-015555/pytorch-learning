{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_he9lhbo",
    "id": "DC31ED3C042A4C68936B8C9C9493C4D4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# [原理](https://blog.csdn.net/hpulfc/article/details/80448570)\n",
    "![](https://img-blog.csdn.net/20180627114128329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hwdWxmYw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。\n",
    "\n",
    "    而语义编码C是由句子Source的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。\n",
    "\n",
    "    如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。\n",
    "\n",
    "    在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。\n",
    "\n",
    "    没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。\n",
    "\n",
    "    上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：\n",
    "\n",
    "    （Tom,0.3）(Chase,0.2) (Jerry,0.5)\n",
    "\n",
    "    每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。\n",
    "\n",
    "    同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。\n",
    "\n",
    "\n",
    "# 注意力机制\n",
    "在“编码器—解码器（seq2seq）”⼀节⾥，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。\n",
    "\n",
    "与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km4dwgf9.PNG?imageView2/0/w/960/h/960)\n",
    "\n",
    "## 注意力机制框架\n",
    "\n",
    "Attention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。$𝐤_𝑖∈ℝ^{𝑑_𝑘}, 𝐯_𝑖∈ℝ^{𝑑_𝑣}$. Query  $𝐪∈ℝ^{𝑑_𝑞}$ , attention layer得到输出与value的维度一致 $𝐨∈ℝ^{𝑑_𝑣}$. 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量$o$则是value的加权求和，而每个key计算的权重与value一一对应。\n",
    "\n",
    "为了计算输出，我们首先假设有一个函数$\\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \\ldots, a_n$ by\n",
    "\n",
    "\n",
    "$$\n",
    "a_i = \\alpha(\\mathbf q, \\mathbf k_i).\n",
    "$$\n",
    "\n",
    "\n",
    "我们使用 softmax函数 获得注意力权重：\n",
    "\n",
    "\n",
    "$$\n",
    "b_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).\n",
    "$$\n",
    "\n",
    "\n",
    "最终的输出就是value的加权求和：\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.\n",
    "$$\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960)\n",
    "\n",
    "不同的attetion layer的区别在于score函数的选择，在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention；随后我们将实现一个引入attention的seq2seq模型并在英法翻译语料上进行训练与测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_lyizjfl",
    "id": "2D4703A032B741F68676C00CF5B06CAF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "graffitiCellId": "id_ucnczrs",
    "id": "60B68D50106A4FBD9CA3C2B863146405",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirs []\n",
      "files ['_about.txt', 'fra.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def file_name_walk(file_dir):\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "#         print(\"root\", root)  # 当前目录路径\n",
    "         print(\"dirs\", dirs)  # 当前路径下所有子目录\n",
    "         print(\"files\", files)  # 当前路径下所有非目录子文件\n",
    "\n",
    "file_name_walk(\"/home/kesci/input/fraeng6506\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bimjldk",
    "id": "4C277595040B4A0689320409E624C366",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Softmax屏蔽\n",
    "在深入研究实现之前，我们首先介绍softmax操作符的一个屏蔽操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "graffitiCellId": "id_39i6msf",
    "id": "CD9DB3F8099F49FB886E0B5029173F44",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SequenceMask(X, X_len,value=-1e6):\n",
    "    maxlen = X.size(1)\n",
    "    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n",
    "    mask = torch.arange((maxlen),dtype=torch.float)[None, :] >= X_len[:, None]   \n",
    "    # print(mask)\n",
    "    X[mask]=value\n",
    "    # print(value)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9242CA9E7BEB457A83FEDB994E53B381",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[       1, -1000000, -1000000],\n",
       "        [       4,        5, -1000000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3], [4,5,6]])\n",
    "SequenceMask(X,torch.tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_mlvtr3b",
    "id": "B0BE96AC54A940878423D3C201ED8477",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_length):\n",
    "    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n",
    "    softmax = nn.Softmax(dim=-1)# 转化为dim+input.dim()+1因为dim = -1\n",
    "    if valid_length is None:\n",
    "        return softmax(X)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_length.dim() == 1:\n",
    "            # print(1)\n",
    "            try:\n",
    "                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n",
    "            except:\n",
    "                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n",
    "        else:\n",
    "            valid_length = valid_length.reshape((-1,))\n",
    "        # fill masked elements with a large negative, whose exp is 0\n",
    "        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n",
    " \n",
    "        return softmax(X).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "graffitiCellId": "id_gjfsirw",
    "id": "2DC35416E322461FB2B9FDD6E0379A02",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3476, 0.6593, 0.8059, 0.0851],\n",
      "         [0.3042, 0.4391, 0.2876, 0.7432]],\n",
      "\n",
      "        [[0.8875, 0.0104, 0.1835, 0.7056],\n",
      "         [0.0315, 0.3148, 0.4189, 0.1169]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4227, 0.5773, 0.0000, 0.0000],\n",
       "         [0.4663, 0.5337, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5234, 0.2177, 0.2589, 0.0000],\n",
       "         [0.2631, 0.3493, 0.3876, 0.0000]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_test = torch.rand((2,2,4),dtype=torch.float)\n",
    "print(number_test)\n",
    "masked_softmax(number_test, torch.FloatTensor([2,3]))\n",
    "# 这里是对于dim=2进行softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_s7xh1br",
    "id": "6E17FB479C8E461185DC7E64B3B51A5D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**超出2维矩阵的乘法** \n",
    "\n",
    "$X$ 和 $Y$ 是维度分别为$(b,n,m)$ 和$(b, m, k)$的张量，进行 $b$ 次二维矩阵乘法后得到 $Z$, 维度为 $(b, n, k)$。\n",
    "\n",
    "\n",
    "$$\n",
    " Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\\qquad for\\ i= 1,…,n\\ .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_c01uiu4",
    "id": "55C627C9AD274B3AA4366F554A7A33C8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3.]],\n",
       "\n",
       "        [[3., 3.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(torch.ones((2,1,3), dtype = torch.float), torch.ones((2,3,2), dtype = torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_5l05rc9",
    "id": "BB8410C97D6143F4885BFA00C8564260",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 点积注意力\n",
    "The dot product 假设query和keys有相同的维度, 即 $\\forall i, 𝐪,𝐤_𝑖 ∈ ℝ_𝑑 $. 通过计算query和key转置的乘积来计算attention score,通常还会除去 $\\sqrt{d}$ 减少计算出来的score对维度𝑑的依赖性，如下\n",
    "\n",
    "\n",
    "$$\n",
    "𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \\sqrt{d} \n",
    "$$\n",
    "\n",
    "假设 $ 𝐐∈ℝ^{𝑚×𝑑}$ 有 $m$ 个query，$𝐊∈ℝ^{𝑛×𝑑}$ 有 $n$ 个keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个score：\n",
    "\n",
    "\n",
    "$$\n",
    "𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\\sqrt{d}\n",
    "$$\n",
    " \n",
    "现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_qpr6dwm",
    "id": "3CB825EFC83C4F7892F0ED1D4038FB49",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to the d2l package.\n",
    "class DotProductAttention(nn.Module): \n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # query: (batch_size, #queries, d)2,1,2\n",
    "    # key: (batch_size, #kv_pairs, d)2,10,2\n",
    "    # value: (batch_size, #kv_pairs, dim_v)2,10,4\n",
    "    # valid_length: either (batch_size, ) or (batch_size, xx)\n",
    "    def forward(self, query, key, value, valid_length=None):\n",
    "        d = query.shape[-1]\n",
    "        # set transpose_b=True to swap the last two dimensions of key\n",
    "        # 小型二维矩阵的转置,就是x,y相乘,这里的x就是1,y就是2\n",
    "        # key.transpose(1,2)行列互换,转置2,2,10\n",
    "        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))# 注意2,1,10\n",
    "        #print(attention_weights.shape)\n",
    "        print(\"attention_weight\\n\",attention_weights)\n",
    "        return torch.bmm(attention_weights, value)#点乘得到注意力ｗｅｉｇｈｔ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_sy1n984",
    "id": "CD3484D8F62449C08842C768228EA444",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 测试\n",
    "现在我们创建了两个批，每个批有一个query和10个key-values对。我们通过valid_length指定，对于第一批，我们只关注前2个键-值对，而对于第二批，我们将检查前6个键-值对。因此，尽管这两个批处理具有相同的查询和键值对，但我们获得的输出是不同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_rt2wg6w",
    "id": "DF454887CCD745858E6505AF13119489",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weight\n",
      " tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = DotProductAttention(dropout=0)#不删除，初始化\n",
    "\n",
    "keys = torch.ones((2,10,2),dtype=torch.float)\n",
    "#print(keys.shape)2,10,2\n",
    "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)# 复制一份\n",
    "#print(values.shape)2,10,4\n",
    "# print(values)\n",
    "atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))# 第一个是关注\n",
    "#print(atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_vqmu7c7",
    "id": "AD0E0FB0D571413E8DE3796462784DD4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 多层感知机注意力\n",
    "在多层感知器中，我们首先将 query and keys 投影到  $ℝ^ℎ$ .为了更具体，我们将可以学习的参数做如下映射 \n",
    "$𝐖_𝑘∈ℝ^{ℎ×𝑑_𝑘}$ ,  $𝐖_𝑞∈ℝ^{ℎ×𝑑_𝑞}$ , and  $𝐯∈ℝ^h$ . 将score函数定义\n",
    "$$\n",
    "𝛼(𝐤,𝐪)=𝐯^𝑇tanh(𝐖_𝑘𝐤+𝐖_𝑞𝐪)\n",
    "$$\n",
    ". \n",
    "然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "graffitiCellId": "id_2pmvrcv",
    "id": "047BD3F3C6DF46AC8C0FBAFF61F34B75",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to the d2l package.多层感知机\n",
    "class MLPAttention(nn.Module):  \n",
    "    def __init__(self, units,ipt_dim,dropout, **kwargs):\n",
    "        super(MLPAttention, self).__init__(**kwargs)\n",
    "        # Use flatten=True to keep query's and key's 3-D shapes.ipt_dim=2,units = 8, dropout=0\n",
    "        self.W_k = nn.Linear(ipt_dim, units, bias=False)# h*d\n",
    "        self.W_q = nn.Linear(ipt_dim, units, bias=False)# h*d\n",
    "        self.v = nn.Linear(units, 1, bias=False)# h\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, valid_length):\n",
    "        query, key = self.W_k(query), self.W_q(key)#\n",
    "        #print(\"size\",query.size(),key.size()) 2,1,8 || 2,10,8\n",
    "        # expand query to (batch_size, #querys, 1, units), and key to\n",
    "        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n",
    "        # torch.squeeze() 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度\n",
    "        # torch.unsqueeze()这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度\n",
    "        # print(query.unsqueeze(2).shape,key.unsqueeze(1).shape)\n",
    "        features = query.unsqueeze(2) + key.unsqueeze(1)# 广播机制\n",
    "        # un\n",
    "        #print(\"features:\",features.size())  #--------------开启\n",
    "        scores = self.v(features).squeeze(-1) \n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n",
    "        return torch.bmm(attention_weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_o2cxg9q",
    "id": "40A0160DE63247F881553CC224705D7A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 测试\n",
    "尽管MLPAttention包含一个额外的MLP模型，但如果给定相同的输入和相同的键，我们将获得与DotProductAttention相同的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "graffitiCellId": "id_ji2koe2",
    "id": "A399CB39F56241048E63FFD6FD3E3AE9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
       "\n",
       "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten = MLPAttention(ipt_dim=2,units = 8, dropout=0)\n",
    "# print(keys.shape)\n",
    "atten(torch.ones((2,1,2), dtype = torch.float), keys, values, torch.FloatTensor([2, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_qqem3ml",
    "id": "C4CD9496F89B418F9420B4ECD6DE4A60",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 总结\n",
    "\n",
    "- 注意力层显式地选择相关的信息。\n",
    "- 注意层的内存由键-值对组成，因此它的输出接近于键类似于查询的值。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_1b80ob1",
    "id": "28BBCA401DCC416385D179EC8A706DE6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 引入注意力机制的Seq2seq模型\n",
    "\n",
    "本节中将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的$t$时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入$D_t$拼接起来一起送到解码器：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800)\n",
    "\n",
    "$$\n",
    "Fig1具有注意机制的seq-to-seq模型解码的第二步\n",
    "$$\n",
    "\n",
    "\n",
    "下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800)\n",
    "\n",
    "$$\n",
    "Fig2具有注意机制的seq-to-seq模型中层结构\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "graffitiCellId": "id_xbbsd1x",
    "id": "5F439000597B4D638C3C6A85BA82A82D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/kesci/input/d2len9900')\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_9bq3vbt",
    "id": "FBF33B6F0ABC45A497E1C94A67392499",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 解码器\n",
    "   由于带有注意机制的seq2seq的编码器与之前章节中的Seq2SeqEncoder相同，所以在此处我们只关注解码器。我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:\n",
    "   \n",
    "- the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values,$H_t$时间序列的隐藏状态\n",
    "- the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state\n",
    "\n",
    "\n",
    "- the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）\n",
    "\n",
    "\n",
    "   在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "graffitiCellId": "id_n3xn2jo",
    "id": "421CF4D81A7E43378AF5C53236DC0586",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2SeqAttentionDecoder(d2l.Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)# 初始化\n",
    "        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)# 隐藏层\n",
    "        # 注意细胞\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens,vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_len, *args):\n",
    "        # encoder层输出:output,隐藏层状态,有效藏毒\n",
    "        outputs, hidden_state = enc_outputs\n",
    "        # print(\"output shape is {}\".format(outputs.shape))\n",
    "        # print(\"output shape is {}\".format(outputs.permute(1,0,-1).shape))\n",
    "#         print(\"first:\",outputs.size(),hidden_state[0].size(),hidden_state[1].size())\n",
    "        # Transpose outputs to (batch_size, seq_len, hidden_size)\n",
    "        return (outputs.permute(1,0,-1), hidden_state, enc_valid_len)\n",
    "        # 因为每个词语,也就是每个单用8\n",
    "        #outputs.swapaxes(0, 1)\n",
    "        \n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, hidden_state, enc_valid_len = state\n",
    "        #(\"X.size\",X.size())\n",
    "        #print(self.embedding(X).shape)\n",
    "        X = self.embedding(X).transpose(0,1)\n",
    "        #print(\"Xembeding.size2\",X.size())\n",
    "        outputs = []\n",
    "        for l, x in enumerate(X):\n",
    "#             print(f\"\\n{l}-th token\")\n",
    "#             print(\"x.first.size()\",x.size())\n",
    "            # query shape: (batch_size, 1, hidden_size)\n",
    "            # select hidden state of the last rnn layer as query\n",
    "            query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1)\n",
    "            # context has same shape as query\n",
    "#             print(\"query enc_outputs, enc_outputs:\\n\",query.size(), enc_outputs.size(), enc_outputs.size())\n",
    "            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)\n",
    "            # Concatenate on the feature dimension\n",
    "#             print(\"context.size:\",context.size())\n",
    "            x = torch.cat((context, x.unsqueeze(1)), dim=-1)\n",
    "            # Reshape x to (1, batch_size, embed_size+hidden_size)\n",
    "#             print(\"rnn\",x.size(), len(hidden_state))\n",
    "            out, hidden_state = self.rnn(x.transpose(0,1), hidden_state)\n",
    "            outputs.append(out)\n",
    "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
    "        return outputs.transpose(0, 1), [enc_outputs, hidden_state,\n",
    "                                        enc_valid_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_wbihjkp",
    "id": "CA550D7B430D4907857B054FC17C77ED",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "现在我们可以用注意力模型来测试seq2seq。为了与第9.7节中的模型保持一致，我们对vocab_size、embed_size、num_hiddens和num_layers使用相同的超参数。结果，我们得到了相同的解码器输出形状，但是状态结构改变了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "graffitiCellId": "id_5u9j8bu",
    "id": "4CCF2FAC8DE741D2B86E509FFA6F33DB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size=4\n",
      "seq_length=7\n",
      "hidden dim=16\n",
      "num_layers=2\n",
      "\n",
      "encoder output size: torch.Size([7, 4, 16])\n",
      "encoder hidden size: torch.Size([2, 4, 16])\n",
      "encoder memory size: torch.Size([2, 4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8,\n",
    "                            num_hiddens=16, num_layers=2)#词典１０个,每个长度为8,隐藏16个,lstm两层\n",
    "# encoder.initialize()\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8,\n",
    "                                  num_hiddens=16, num_layers=2)\n",
    "X = torch.zeros((4, 7),dtype=torch.long)# 4个句子,每个句子7个单词,但是到后面会转置\n",
    "print(\"batch size=4\\nseq_length=7\\nhidden dim=16\\nnum_layers=2\\n\")\n",
    "# batch size=4\n",
    "# seq_length=7\n",
    "# hidden dim=16\n",
    "# num_layers=2\n",
    "print('encoder output size:', encoder(X)[0].size())\n",
    "print('encoder hidden size:', encoder(X)[1][0].size())\n",
    "print('encoder memory size:', encoder(X)[1][1].size())\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "out, state = decoder(X, state)\n",
    "out.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_k53lcnp",
    "id": "25FB3334E1684E59966D79EA35B050D0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 训练\n",
    "\n",
    "  与第9.7.4节相似，通过应用相同的训练超参数和相同的训练损失来尝试一个简单的娱乐模型。从结果中我们可以看出，由于训练数据集中的序列相对较短，额外的注意层并没有带来显著的改进。由于编码器和解码器的注意层的计算开销，该模型比没有注意的seq2seq模型慢得多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "graffitiCellId": "id_3uv6guo",
    "id": "6DC0F075B2EA4EA78B6D405A35D896CC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import torch\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torch.utils import data\n",
    "import sys\n",
    "import collections\n",
    "# 下面这些函数是上一节写好的\n",
    "class Vocab(object): # This class is saved in d2l.\n",
    "  def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n",
    "    # sort by frequency and token\n",
    "    counter = collections.Counter(tokens)\n",
    "    token_freqs = sorted(counter.items(), key=lambda x: x[0])\n",
    "    token_freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "    if use_special_tokens:\n",
    "      # padding, begin of sentence, end of sentence, unknown\n",
    "      self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n",
    "      tokens = ['', '', '', '']\n",
    "    else:\n",
    "      self.unk = 0\n",
    "      tokens = ['']\n",
    "    tokens += [token for token, freq in token_freqs if freq >= min_freq]\n",
    "    self.idx_to_token = []\n",
    "    self.token_to_idx = dict()\n",
    "    for token in tokens:\n",
    "      self.idx_to_token.append(token)\n",
    "      self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "      \n",
    "  def __len__(self):\n",
    "    return len(self.idx_to_token)\n",
    "  \n",
    "  def __getitem__(self, tokens):\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "      return self.token_to_idx.get(tokens, self.unk)\n",
    "    else:\n",
    "      return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "  def to_tokens(self, indices):\n",
    "    if not isinstance(indices, (list, tuple)):\n",
    "      return self.idx_to_token[indices]\n",
    "    else:\n",
    "      return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "def load_data_nmt(batch_size, max_len, num_examples=1000):\n",
    "    \"\"\"Download an NMT dataset, return its vocabulary and data iterator.\"\"\"\n",
    "    # Download and preprocess\n",
    "    def preprocess_raw(text):\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        out = ''\n",
    "        for i, char in enumerate(text.lower()):\n",
    "            if char in (',', '!', '.') and text[i-1] != ' ':\n",
    "                out += ' '\n",
    "            out += char\n",
    "        return out \n",
    "\n",
    "\n",
    "    with open('/home/kesci/input/fraeng6506/fra.txt', 'r') as f:\n",
    "      raw_text = f.read()\n",
    "\n",
    "\n",
    "    text = preprocess_raw(raw_text)\n",
    "\n",
    "    # Tokenize\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if i >= num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "\n",
    "    # Build vocab\n",
    "    def build_vocab(tokens):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "        return Vocab(tokens, min_freq=3, use_special_tokens=True)\n",
    "    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n",
    "\n",
    "    # Convert to index arrays\n",
    "    def pad(line, max_len, padding_token):\n",
    "        if len(line) > max_len:\n",
    "            return line[:max_len]\n",
    "        return line + [padding_token] * (max_len - len(line))\n",
    "\n",
    "    def build_array(lines, vocab, max_len, is_source):\n",
    "        lines = [vocab[line] for line in lines]\n",
    "        if not is_source:\n",
    "            lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n",
    "        array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n",
    "        valid_len = (array != vocab.pad).sum(1)\n",
    "        return array, valid_len\n",
    "\n",
    "    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n",
    "    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n",
    "    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n",
    "    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "    return src_vocab, tgt_vocab, train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "graffitiCellId": "id_uwt29qm",
    "id": "16B6DBF49DE6462CB969E7FEB2B80EA4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, ctx = 0.005, 500, d2l.try_gpu()\n",
    "\n",
    "src_vocab, tgt_vocab, train_iter = load_data_nmt(batch_size, num_steps)\n",
    "encoder = d2l.Seq2SeqEncoder(\n",
    "    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqAttentionDecoder(\n",
    "    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = d2l.EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_l19wr5o",
    "id": "B087A130ADCD4C6A855201B68C1B437F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "graffitiCellId": "id_l50r9v2",
    "id": "C85DB18574124B778FF37F0C939A87CF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   50,loss 0.098, time 41.8 sec\n",
      "epoch  100,loss 0.046, time 41.9 sec\n",
      "epoch  150,loss 0.032, time 41.9 sec\n",
      "epoch  200,loss 0.027, time 41.9 sec\n",
      "epoch  250,loss 0.028, time 42.3 sec\n",
      "epoch  300,loss 0.024, time 42.3 sec\n",
      "epoch  350,loss 0.024, time 42.0 sec\n",
      "epoch  400,loss 0.024, time 42.5 sec\n",
      "epoch  450,loss 0.023, time 45.6 sec\n",
      "epoch  500,loss 0.023, time 42.1 sec\n"
     ]
    }
   ],
   "source": [
    "d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "graffitiCellId": "id_fgsubff",
    "id": "59CD0BDF632440938BF388ACCD017A09",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go . => va !\n",
      "Good Night ! => à pas  .\n",
      "I'm OK . => je vais bien .\n",
      "I won ! => je l'ai emporté !\n"
     ]
    }
   ],
   "source": [
    "for sentence in ['Go .', 'Good Night !', \"I'm OK .\", 'I won !']:\n",
    "    print(sentence + ' => ' + d2l.predict_s2s_ch9(\n",
    "        model, sentence, src_vocab, tgt_vocab, num_steps, ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2400C61CD3824675A36DED3276312263",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/kesci/work', '/opt/conda/lib/python37.zip', '/opt/conda/lib/python3.7', '/opt/conda/lib/python3.7/lib-dynload', '', '/opt/conda/lib/python3.7/site-packages', '/opt/conda/lib/python3.7/site-packages/papermill-7.7.7-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/tenacity-6.0.0-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/nbformat-4.4.0-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/future-0.18.2-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/Click-7.0-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/ansiwrap-0.8.4-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/defusedxml-0.6.0-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/pandocfilters-1.4.2-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/ipython_genutils-0.2.0-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/textwrap3-0.9.2-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/webencodings-0.5.1-py3.7.egg', '/opt/conda/lib/python3.7/site-packages/IPython/extensions', '/home/kesci/.ipython', '/home/kesci/input/d2len9900']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "E550D3030FC742378D05C6A7EB748194",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9951FB186D42498485211DEC7D223D8D",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
