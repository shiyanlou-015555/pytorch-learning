{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_j7o1a6n",
    "id": "69262AB48D5C40E0893D46893CFE9B60",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## course content\n",
    "\n",
    "1. lenet 模型介绍\n",
    "2. lenet 网络搭建\n",
    "3. 运用lenet进行图像识别-fashion-mnist数据集\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_ajekwu7",
    "id": "Z4Mfv4K6dbhQ",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#  Convolutional Neural Networks\n",
    "\n",
    "使用全连接层的局限性：\n",
    "\n",
    "- 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。\n",
    "- 对于大尺寸的输入图像，使用全连接层容易导致模型过大。\n",
    "\n",
    "使用卷积层的优势：\n",
    "\n",
    "- 卷积层保留输入形状。\n",
    "- 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_kpncnr4",
    "id": "DCCD52CB4C38495789BA2C76F01D4332",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LeNet 模型\n",
    "\n",
    "LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5ndwsmsao.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。\n",
    "\n",
    "卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。\n",
    "\n",
    "全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n",
    "\n",
    "下面我们通过Sequential类来实现LeNet模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "graffitiCellId": "id_r0mf24i",
    "id": "DHAr1_2J1jMC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Using cached https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from torchtext)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from torchtext)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchtext)\n",
      "Collecting sentencepiece (from torchtext)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 4.3kB/s ta 0:00:026\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchtext)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchtext)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.85 torchtext-0.5.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "!pip install torchtext\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\")\n",
    "import d2lzh1981 as d2l\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "graffitiCellId": "id_jxhddby",
    "id": "hp3vLYIbdbhd",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#net\n",
    "class Flatten(torch.nn.Module):  #展平操作\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Reshape(torch.nn.Module): #将图像大小重定型\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,1,28,28)      #(B x C x H x W)\n",
    "net = torch.nn.Sequential(     #Lelet                                                  \n",
    "    Reshape(),\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #b*1*28*28  =>b*6*28*28\n",
    "    nn.Sigmoid(),                      # 激活函数                                 \n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*6*28*28  =>b*6*14*14\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),           #b*6*14*14  =>b*16*10*10\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*16*10*10  => b*16*5*5\n",
    "    Flatten(),                                                          #b*16*5*5   => b*400\n",
    "    nn.Linear(in_features=16*5*5, out_features=120),# 第一个全连接层\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_00eauaf",
    "id": "OnaREs_Zdbhg",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "接下来我们构造一个高和宽均为28的单通道数据样本，并逐层进行前向计算来查看每个层的输出形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "graffitiCellId": "id_l3of2c4",
    "id": "yiamwgtvdbhh",
    "jupyter": {},
    "outputId": "88be586b-b793-48b7-f4e2-7e598cecd22e",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape output shape: \t torch.Size([1, 1, 28, 28])\n",
      "Conv2d output shape: \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape: \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape: \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape: \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape: \t torch.Size([1, 400])\n",
      "Linear output shape: \t torch.Size([1, 120])\n",
      "Sigmoid output shape: \t torch.Size([1, 120])\n",
      "Linear output shape: \t torch.Size([1, 84])\n",
      "Sigmoid output shape: \t torch.Size([1, 84])\n",
      "Linear output shape: \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "#print\n",
    "X = torch.randn(size=(1,1,28,28), dtype = torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t',X.shape)\n",
    "    # 这里处理数据的所有过程"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_0ppp3nj",
    "id": "fzicoTnEdbhl",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看到，在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5ndxi6jl5.png?imageView2/0/w/640/h/640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_p1kzp1a",
    "id": "BnKUIw6Ddbhr",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 获取数据和训练模型\n",
    "\n",
    "下面我们来实现LeNet模型。我们仍然使用Fashion-MNIST作为训练数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "graffitiCellId": "id_f0dgnij",
    "id": "ECMwOKTzdbhr",
    "jupyter": {},
    "outputId": "9a7f2ac9-c95b-4bbb-e902-dcfbfd16301a",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "# 数据\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(\n",
    "    batch_size=batch_size, root='/home/kesci/input/FashionMNIST2065')\n",
    "print(len(train_iter))\n",
    "# 每批数据256个\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_uge3tcz",
    "id": "D38DDA51EF0D4AFE809003A1686CDA11",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "为了使读者更加形象的看到数据，添加额外的部分来展示数据的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_d5nld6i",
    "id": "4FE5CE6E20494BFE898E9D8EAAF30C7B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 9\n",
      "torch.Size([1, 28, 28]) 6\n",
      "torch.Size([1, 28, 28]) 8\n",
      "torch.Size([1, 28, 28]) 3\n",
      "torch.Size([1, 28, 28]) 0\n",
      "torch.Size([1, 28, 28]) 7\n",
      "torch.Size([1, 28, 28]) 6\n",
      "torch.Size([1, 28, 28]) 3\n",
      "torch.Size([1, 28, 28]) 6\n",
      "torch.Size([1, 28, 28]) 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/4FE5CE6E20494BFE898E9D8EAAF30C7B/q5vyvczxn9.svg\">"
      ],
      "text/plain": [
       "<Figure size 864x864 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#数据展示\n",
    "import matplotlib.pyplot as plt\n",
    "def show_fashion_mnist(images, labels):\n",
    "    d2l.use_svg_display()\n",
    "    # 这里的_表示我们忽略（不使用）的变量\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.view((28, 28)).numpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "for Xdata,ylabel in train_iter:\n",
    "    break\n",
    "X, y = [], []\n",
    "for i in range(10):\n",
    "    print(Xdata[i].shape,ylabel[i].numpy())\n",
    "    X.append(Xdata[i]) # 将第i个feature加到X中\n",
    "    y.append(ylabel[i].numpy()) # 将第i个label加到y中\n",
    "show_fashion_mnist(X, y)\n",
    "#以前的函数进行展示用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_qw3jw30",
    "id": "Gg8uZka5dbhu",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。我们查看看是否可以用GPU，如果成功则使用`cuda:0`，否则仍然使用`cpu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "graffitiCellId": "id_xnv8h9s",
    "id": "Y3zs_KyHdbhv",
    "jupyter": {},
    "outputId": "da77a9d2-55c9-4781-8132-7ebba7f6f3f6",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function has been saved in the d2l package for future use\n",
    "#use GPU\n",
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = try_gpu()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_t2kmxh8",
    "id": "nhhHxEjVdbhz",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们实现`evaluate_accuracy`函数，该函数用于计算模型`net`在数据集`data_iter`上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "graffitiCellId": "id_x6dxfdo",
    "id": "g4iEi1U8dbh0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#计算准确率\n",
    "'''\n",
    "(1). net.train()\n",
    "  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True\n",
    "(2). net.eval()\n",
    "不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False\n",
    "'''\n",
    "\n",
    "def evaluate_accuracy(data_iter, net,device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    acc_sum,n = torch.tensor([0],dtype=torch.float32,device=device),0\n",
    "    for X,y in data_iter:\n",
    "        # If device is the GPU, copy the data to the GPU.\n",
    "        X,y = X.to(device),y.to(device)\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))  #[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] => [ 4 , 2 ]\n",
    "            n += y.shape[0]\n",
    "    return acc_sum.item()/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_ccpg4oo",
    "id": "qZj0UJ91dbh3",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们定义函数`train_ch5`，用于训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "graffitiCellId": "id_e22sitj",
    "id": "WQ8PZBb4dbh4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#训练函数\n",
    "def train_ch5(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', device)\n",
    "    net.to(device)# 将net放入gpu,准备训练\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n",
    "        train_acc_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n",
    "        # 将数据放入ｇｐｕ\n",
    "        n, start = 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            net.train()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            X,y = X.to(device),y.to(device) # 有关数据全部放入gpu\n",
    "            y_hat = net(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y = y.long()\n",
    "                train_l_sum += loss.float()\n",
    "                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n",
    "                n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net,device)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc,\n",
    "                 time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_o8t94tl",
    "id": "QCTjn-iRdbh6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们重新将模型参数初始化到对应的设备`device`(`cpu` or `cuda:0`)之上，并使用Xavier随机初始化。损失函数和训练算法则依然使用交叉熵损失函数和小批量随机梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "graffitiCellId": "id_k8iyyir",
    "id": "SRwnEh4ndbh7",
    "jupyter": {},
    "outputId": "abcc9495-ed33-41a2-c6f5-7bcb8b82c3e4",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch 1, loss 0.0087, train acc 0.147, test acc 0.454, time 5.3 sec\n",
      "epoch 2, loss 0.0042, train acc 0.569, test acc 0.646, time 5.3 sec\n",
      "epoch 3, loss 0.0031, train acc 0.693, test acc 0.708, time 5.3 sec\n",
      "epoch 4, loss 0.0026, train acc 0.734, test acc 0.712, time 5.3 sec\n",
      "epoch 5, loss 0.0024, train acc 0.759, test acc 0.752, time 5.3 sec\n",
      "epoch 6, loss 0.0022, train acc 0.779, test acc 0.756, time 5.3 sec\n",
      "epoch 7, loss 0.0021, train acc 0.796, test acc 0.790, time 5.3 sec\n",
      "epoch 8, loss 0.0020, train acc 0.809, test acc 0.790, time 5.3 sec\n",
      "epoch 9, loss 0.0019, train acc 0.821, test acc 0.812, time 5.3 sec\n",
      "epoch 10, loss 0.0018, train acc 0.829, test acc 0.804, time 5.3 sec\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "lr, num_epochs = 0.9, 10\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "net = net.to(device)\n",
    "# 定义weight_init函数，并在weight_init中通过判断模块的类型来进行不同的参数初始化定义类型。\n",
    "# model=Net(…) 创建网络结构\n",
    "# model.apply(weight_init),将weight_init初始化方式应用到submodels上\n",
    "criterion = nn.CrossEntropyLoss()   #交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近\n",
    "train_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "graffitiCellId": "id_y3tdhvt",
    "id": "6C2C48F49CD24FB38F6B313B6EC1983F",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n",
      "tensor([9, 2, 1, 1, 6, 1, 2, 6, 5, 7], device='cuda:0')\n",
      "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for testdata,testlabe in test_iter:\n",
    "    testdata,testlabe = testdata.to(device),testlabe.to(device)\n",
    "    break\n",
    "print(testdata.shape,testlabe.shape)\n",
    "net.eval()\n",
    "y_pre = net(testdata)\n",
    "print(torch.argmax(y_pre,dim=1)[:10])\n",
    "print(testlabe[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "graffitiCellId": "id_xwmnzx5",
    "id": "pRUx8zxq1XmD",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 总结：\n",
    "\n",
    "卷积神经网络就是含卷积层的网络。\n",
    "LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。\n",
    "- 池化层有参与模型的正向计算，同样也会参与反向传播\n",
    "- 池化层直接对窗口内的元素求最大值或平均值，并没有模型参数参与计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8F512E07CDC439793806BD06E7FE23B",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
